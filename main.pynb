{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61wql2uihhYW",
        "outputId": "3842bc6e-291d-4b8b-a280-7e2f4b0b66a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.4.2)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuGQj0cQcGBS",
        "outputId": "ebd52b42-fa01-4d65-aed2-8c78bcfc3c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.6.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoMjGwowM_-j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception, ResNet50V2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import clone_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBEhqPaYNYW4",
        "outputId": "e53c2953-ef63-488b-b4b6-99fb896e6408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW76bGu6M_-m"
      },
      "outputs": [],
      "source": [
        "base_dir = '/content/drive/MyDrive/dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1nDMQkmM_-o"
      },
      "outputs": [],
      "source": [
        "img_height, img_width = 224, 224\n",
        "batch_size = 30\n",
        "epochs = 5\n",
        "cls_epochs = 100\n",
        "latent_dim = 512\n",
        "cls_model_count = 50\n",
        "subset_fraction = 0.8\n",
        "subset_size = int(latent_dim * subset_fraction)\n",
        "learning_rate = 1e-3\n",
        "cls_learning_rate = 1e-1\n",
        "cls_depth = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpbHbZxnM_-q",
        "outputId": "70ef1b7b-116e-46f9-a9f4-60588e737a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8640 images belonging to 12 classes.\n",
            "Found 960 images belonging to 12 classes.\n",
            "Found 2400 images belonging to 12 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    zoom_range=0.2,\n",
        "    validation_split=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    seed=1337,\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    seed=1337,\n",
        "    shuffle=True,\n",
        "    subset='validation',\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(train_generator.filepaths).intersection(val_generator.filepaths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y8OaZ_Q7HgE",
        "outputId": "b7ae5ca6-9e3c-45ca-a1c2-92bc9b10ced9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))"
      ],
      "metadata": {
        "id": "37CNjht2dYxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ZuXUDuM_-r"
      },
      "source": [
        "İLK DENEME (Burada Xception ve XGBoost'u birleştirmeye çalıştım. Sadece arka arkaya değil, aklımda XGBoost'un loss skoruyla tüm modelin ağırlıklarını güncellemek vardı. Sadece XGBoost değil, özellik çıkarımı için kullandığım Xception da güncellenecekti ama çok uyumsuzluk çıktı iki kütüphane arasında o yüzden yapamadım hocam.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD_BDlLNM_-w"
      },
      "outputs": [],
      "source": [
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUxY9-gdM_-y"
      },
      "outputs": [],
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(latent_dim, activation='linear')(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYMqU0ZBM_-1"
      },
      "outputs": [],
      "source": [
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nObpYOliM_-2"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQnfzlDpM_-5"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "train_acc_metric = CategoricalAccuracy()\n",
        "val_acc_metric = CategoricalAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smKvI6xBM_-6"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7lss9Y3M_-8"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLIYcctDM_-9"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jQoQvxQM_-9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        if step % 5 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(train_generator):\n",
        "            train_generator.on_epoch_end()\n",
        "            break\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
        "\n",
        "    train_acc_metric.reset_state()\n",
        "\n",
        "    for x_batch_val, y_batch_val in val_generator:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
        "    print(f\"Time taken: {time.time() - start_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srn5YntvM_--"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"categorical_accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_adQmfeiM_--"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "4fTzL5vcddYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "İKİNCİ DENEME (Tf'nin native olarak ensemble algoritmalarını desteklediğini öğrendikten sonra onları denemek istedim hocam ama burada da stacking çalışmadığını gördüm uzun bir süre farklı yollar denedikten sonra.)"
      ],
      "metadata": {
        "id": "VGQQ927MdfuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = tfdf.keras.RandomForestModel()"
      ],
      "metadata": {
        "id": "gIe8i1Eudmx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "Iva7bDWLdoCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "MSguN0zFdpS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "bONYN3MJfNtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÜÇÜNCÜ DENEME (Burada keras kullanılarak türetilen küçük classification modelleri üretip bootstrapping ile kendi ensemble modelimi kendim oluşturmak istedim ama her bir öğrenici tarafından üretilen gradient'leri bir türlü CNN modelini güncellemek için birleştiremedim.)"
      ],
      "metadata": {
        "id": "q_mfvQCmfPut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "latent_vector = Dense(latent_dim, activation='linear')(x)"
      ],
      "metadata": {
        "id": "vJkEM_EFfs_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=latent_vector)"
      ],
      "metadata": {
        "id": "KKGp6kxmp7_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "ojUaGiENpRus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.001)\n",
        "train_acc_metric = CategoricalAccuracy()\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "BYs58gaJpUXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = Sequential()\n",
        "cls_model.add(Input(shape=(subset_size,)))\n",
        "cls_model.add(Dense(1024, activation='relu'))\n",
        "cls_model.add(Dense(512, activation='relu'))\n",
        "cls_model.add(Dense(256, activation='relu'))\n",
        "cls_model.add(Dense(64, activation='relu'))\n",
        "cls_model.add(Dense(train_generator.num_classes, activation='softmax'))\n",
        "\n",
        "def create_clone_models_with_opt_and_loss():\n",
        "    return [clone_model(cls_model) for _ in range(cls_model_count)], \\\n",
        "           [Adam(learning_rate=learning_rate) for _ in range(cls_model_count)], \\\n",
        "           [CategoricalCrossentropy() for _ in range(cls_model_count)]"
      ],
      "metadata": {
        "id": "nqpCzIgNg5zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_model_on_subset(model, loss, X, y):\n",
        "    indices = np.random.choice(latent_dim, subset_size, replace=False)\n",
        "\n",
        "    X_subset = tf.gather(X, indices, axis=1)\n",
        "    y_subset = y\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(X_subset, training=True)\n",
        "        loss_value = loss(y_subset, logits)\n",
        "        return tape, logits, loss_value"
      ],
      "metadata": {
        "id": "JSKyiGjAiXrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_model_on_subset(model, loss, X, y):\n",
        "    indices = np.random.choice(latent_dim, subset_size, replace=False)\n",
        "\n",
        "    X_subset = tf.gather(X, indices, axis=1)\n",
        "    y_subset = y\n",
        "\n",
        "    logits = model(X_subset, training=True)\n",
        "    loss_value = loss(y_subset, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "XEldYpzjSlaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(models, opts, losses, X, y):\n",
        "    tapes = []\n",
        "    logits = []\n",
        "    pred_losses = []\n",
        "    # latent_vector_grads = []\n",
        "    with tf.GradientTape() as tape:\n",
        "        latent_vector = model(X, training=True)\n",
        "\n",
        "        for index in range(cls_model_count):\n",
        "            cls_tape, cls_logits, cls_loss = train_model_on_subset(models[index], losses[index], latent_vector, y)\n",
        "            tapes.append(cls_tape)\n",
        "            logits.append(cls_logits)\n",
        "            pred_losses.append(cls_loss)\n",
        "\n",
        "    for index in range(cls_model_count):\n",
        "        cls_grads = tapes[index].gradient(pred_losses[index], models[index].trainable_weights)\n",
        "        opts[index].apply_gradients(zip(cls_grads, models[index].trainable_weights))\n",
        "\n",
        "        # latent_vector_grads.append(tapes[index].gradient(pred_losses[index], latent_vector))\n",
        "\n",
        "    train_acc_metric.update_state(y, tf.reduce_mean(logits, axis=0))\n",
        "\n",
        "    # avg_latent_vector_grad = tf.reduce_mean(latent_vector_grads, axis=0)\n",
        "\n",
        "    # grads = tape.gradient(latent_vector, model.trainable_weights, output_gradients=avg_latent_vector_grad)\n",
        "    # optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    return tf.reduce_mean(pred_losses, axis=0)"
      ],
      "metadata": {
        "id": "7GyUpTcNjBqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(models, losees, X, y):\n",
        "    logits = []\n",
        "    pred_losses = []\n",
        "\n",
        "    latent_vector = model(X, training=False)\n",
        "\n",
        "    for index in range(cls_model_count):\n",
        "        cls_logits, cls_loss = test_model_on_subset(models[index], losses[index], latent_vector, y)\n",
        "        logits.append(cls_logits)\n",
        "        pred_losses.append(cls_loss)\n",
        "\n",
        "    val_acc_metric.update_state(y, tf.reduce_mean(logits, axis=0))\n",
        "\n",
        "    return tf.reduce_mean(pred_losses, axis=0)"
      ],
      "metadata": {
        "id": "zqGlt5rOjDzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models, opts, losses = create_clone_models_with_opt_and_loss()"
      ],
      "metadata": {
        "id": "rQXGyNzGqCgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uR6wqniqt7eJ",
        "outputId": "31885c20-bf18-44da-8016-6537157f0b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1171"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []"
      ],
      "metadata": {
        "id": "ncTahOWOY2fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss_value_for_epoch = None\n",
        "    val_loss_value_for_epoch = None\n",
        "    for step, (X_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        loss_value = train_step(models, opts, losses, X_batch_train, y_batch_train)\n",
        "\n",
        "\n",
        "        if step % 25 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(train_generator):\n",
        "            train_generator.on_epoch_end()\n",
        "            train_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    train_loss_history.append(train_loss_value_for_epoch)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_state()\n",
        "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
        "\n",
        "    train_accuracy_history.append(train_acc)\n",
        "\n",
        "    for step, (X_batch_val, y_batch_val) in enumerate(val_generator):\n",
        "        loss_value = test_step(models, losses, x_batch_val, y_batch_val)\n",
        "\n",
        "        print(\n",
        "            f\"Validation loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "        )\n",
        "        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(val_generator):\n",
        "            val_generator.on_epoch_end()\n",
        "            val_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    val_loss_history.append(val_loss_value_for_epoch)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
        "    print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    val_accuracy_history.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uEU4Fw_AjFrW",
        "outputId": "0f75066d-0945-45a6-c547-e02884951fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for 1 batch) at step 0: 2.5783\n",
            "Seen so far: 32 samples\n",
            "Training loss (for 1 batch) at step 25: 12.0782\n",
            "Seen so far: 832 samples\n",
            "Training loss (for 1 batch) at step 50: 5.2677\n",
            "Seen so far: 1632 samples\n",
            "Training loss (for 1 batch) at step 75: 3.2382\n",
            "Seen so far: 2432 samples\n",
            "Training loss (for 1 batch) at step 100: 2.8138\n",
            "Seen so far: 3232 samples\n",
            "Training loss (for 1 batch) at step 125: 2.1180\n",
            "Seen so far: 4032 samples\n",
            "Training loss (for 1 batch) at step 150: 2.0749\n",
            "Seen so far: 4832 samples\n",
            "Training loss (for 1 batch) at step 175: 1.6448\n",
            "Seen so far: 5632 samples\n",
            "Training loss (for 1 batch) at step 200: 1.3250\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for 1 batch) at step 225: 4.4367\n",
            "Seen so far: 7232 samples\n",
            "Training loss (for 1 batch) at step 250: 3.6593\n",
            "Seen so far: 8032 samples\n",
            "Training acc over epoch: 0.2608\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for 1 batch) at step 269: 1.5633\n",
            "Seen so far: 8640 samples\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-97909412783d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training acc over epoch: {float(train_acc):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         print(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, x, transform_parameters)\u001b[0m\n\u001b[1;32m   2010\u001b[0m         \u001b[0mimg_channel_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_axis\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2012\u001b[0;31m         x = apply_affine_transform(\n\u001b[0m\u001b[1;32m   2013\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m             \u001b[0mtransform_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"theta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36mapply_affine_transform\u001b[0;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[1;32m   2608\u001b[0m         \u001b[0mfinal_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2610\u001b[0;31m         channel_images = [\n\u001b[0m\u001b[1;32m   2611\u001b[0m             ndimage.interpolation.affine_transform(\n\u001b[1;32m   2612\u001b[0m                 \u001b[0mx_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2610\u001b[0m         channel_images = [\n\u001b[0;32m-> 2611\u001b[0;31m             ndimage.interpolation.affine_transform(\n\u001b[0m\u001b[1;32m   2612\u001b[0m                 \u001b[0mx_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2613\u001b[0m                 \u001b[0mfinal_affine_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/ndimage/_interpolation.py\u001b[0m in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    612\u001b[0m                              mode, cval, npad, False)\n\u001b[1;32m    613\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         _nd_image.geometric_transform(filtered, None, None, matrix, offset,\n\u001b[0m\u001b[1;32m    615\u001b[0m                                       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m                                       None)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "GPDMktEAbsRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DÖRDÜNCÜ DENEME (Burada keras yerine CatBoost kullanmak istedim. CNN modeli latent vektör çıkışı verecekti ve ben o vektörü alıp CatBoost'a input olarak verecektim. Bu model sorunsuz çalıştı aslında, 0.83'lere çıkan doğruluk da verdi bana ama CatBoost ile CNN modeli arasında bir bağlantı olmadığı için CNN üzerinde fine tuning yapamadım.)"
      ],
      "metadata": {
        "id": "RK50E3f_bwpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "0OEBDZJziaM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "latent_vector = Dense(latent_dim, activation='linear')(x)"
      ],
      "metadata": {
        "id": "POAIGTMEcL_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=latent_vector)"
      ],
      "metadata": {
        "id": "6DXnfkWncOjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "mL9iEbwQcR4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "train_acc_metric = CategoricalAccuracy()\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "RPUiIbkGcR25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool, sum_models"
      ],
      "metadata": {
        "id": "9wjE7hDUfZ_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = CatBoostClassifier(\n",
        "    iterations=cls_epochs,\n",
        "    learning_rate=cls_learning_rate,\n",
        "    depth=cls_depth,\n",
        "    classes_count=train_generator.num_classes,\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "79WnBhx9c5Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_time = True"
      ],
      "metadata": {
        "id": "BITDF0esO6CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "# def train_model_on_subset(X, y):\n",
        "#     global first_time\n",
        "\n",
        "#     y_argmax = np.argmax(y, axis=1)\n",
        "#     batch_data = Pool(X, label=y_argmax)\n",
        "\n",
        "#     print(X.shape)\n",
        "#     print(y_argmax.shape)\n",
        "\n",
        "#     if first_time:\n",
        "#       cls_model.fit(X=batch_data)\n",
        "#       first_time = False\n",
        "#     else:\n",
        "#       batch_data.set_baseline(cls_model.predict(batch_data, prediction_type=\"Probability\", verbose=0))\n",
        "#       cls_temp_model.fit(X=batch_data)\n",
        "#       model = sum_models([cls_model, cls_temp_model])\n",
        "\n",
        "#     logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "#     loss_value = loss_fn(y, logits)\n",
        "#     return logits, loss_value"
      ],
      "metadata": {
        "id": "AtC25dNFa4VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_model_on_subset(X, y):\n",
        "    global first_time\n",
        "\n",
        "    y_argmax = np.argmax(y, axis=1)\n",
        "\n",
        "    if first_time:\n",
        "      cls_model.fit(X, y_argmax)\n",
        "      first_time = False\n",
        "    else:\n",
        "      cls_model.fit(X, y_argmax, init_model=cls_model)\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "I-R8mdRpcaPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_model_on_subset(X, y):\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "iDzqOmyHgOhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(X, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        latent_vector = model(X, training=False)\n",
        "        np_latent_vector = latent_vector.numpy()\n",
        "        # np_latent_vector = np_latent_vector.astype(np.float32)\n",
        "\n",
        "        logits, loss_value = train_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "        train_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "EqrC7-0Kc1-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(X, y):\n",
        "    latent_vector = model(X, training=False)\n",
        "    np_latent_vector = latent_vector.numpy()\n",
        "    # np_latent_vector = np_latent_vector.astype(np.float32)\n",
        "\n",
        "    logits, loss_value = test_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "    val_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "ScMiuvo1gNe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF-5CbGujcUZ",
        "outputId": "16a83894-1a8d-4b6a-ce63-c77712275b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []"
      ],
      "metadata": {
        "id": "TLTM7-wyw-rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss_value_for_epoch = None\n",
        "    val_loss_value_for_epoch = None\n",
        "    for step, (X_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        loss_value = train_step(X_batch_train, y_batch_train)\n",
        "\n",
        "        if step % 25 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(train_generator):\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at last step: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "            train_generator.on_epoch_end()\n",
        "            train_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    train_loss_history.append(train_loss_value_for_epoch)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_state()\n",
        "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
        "\n",
        "    train_accuracy_history.append(train_acc)\n",
        "\n",
        "    for step, (X_batch_val, y_batch_val) in enumerate(val_generator):\n",
        "        loss_value = test_step(X_batch_val, y_batch_val)\n",
        "\n",
        "        print(\n",
        "            f\"Validation loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "        )\n",
        "        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(val_generator):\n",
        "            val_generator.on_epoch_end()\n",
        "            val_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    val_loss_history.append(val_loss_value_for_epoch)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
        "    print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    val_accuracy_history.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7GQRJChgs9f",
        "outputId": "81066d50-cd8e-4eb5-bbb6-6f27b26b9a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for 1 batch) at step 0: 1.4677\n",
            "Seen so far: 80 samples\n",
            "Training loss (for 1 batch) at step 25: 1.1843\n",
            "Seen so far: 2080 samples\n",
            "Training loss (for 1 batch) at step 50: 1.0227\n",
            "Seen so far: 4080 samples\n",
            "Training loss (for 1 batch) at step 75: 1.0406\n",
            "Seen so far: 6080 samples\n",
            "Training loss (for 1 batch) at step 100: 1.2103\n",
            "Seen so far: 8080 samples\n",
            "Training loss (for 1 batch) at last step: 1.2582\n",
            "Seen so far: 8640 samples\n",
            "Training acc over epoch: 0.6781\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[0], expected a dimension of 1, got 80 [Op:Squeeze] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-87669c519f31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         print(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-60d73c69fdc6>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_on_subset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_latent_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mval_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_operations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                     \u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 )\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         )\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         sample_weight = losses_utils.apply_valid_mask(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/metrics/accuracy_metrics.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical_accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         super().__init__(\n\u001b[0;32m--> 169\u001b[0;31m             lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             ),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\u001b[0m in \u001b[0;36msparse_categorical_matches\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m     ):\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0mreshape_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Squeeze_device_/job:localhost/replica:0/task:0/device:CPU:0}} Can not squeeze dim[0], expected a dimension of 1, got 80 [Op:Squeeze] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "3iLtDGZL_gxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BEŞİNCİ DENEME (Burada 4. denemedeki modelin aynısını kullandım ama gradient'leri yaklaşık olarak kendim üretmek istedim ki CNN modelini fine tune edebileyim ama burada da tensorflow gradient tape'in CatBoost ile bir bağlantısı olmadığı için doğru gradient'leri CNN tarafında üretemedim. O yüzden burası çalışamadı.)"
      ],
      "metadata": {
        "id": "5y54DoaH_dhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "YegQjsG7MLKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "latent_vector = Dense(latent_dim, activation='linear')(x)"
      ],
      "metadata": {
        "id": "sRR-rVkd_q9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=latent_vector)"
      ],
      "metadata": {
        "id": "ycsaYX4g_vZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "n_F-bBQi_xhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "train_acc_metric = CategoricalAccuracy()\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "VnoPYcv0_0V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool, sum_models"
      ],
      "metadata": {
        "id": "dZNeyqnP_6AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = CatBoostClassifier(\n",
        "    iterations=cls_epochs,\n",
        "    learning_rate=cls_learning_rate,\n",
        "    depth=cls_depth,\n",
        "    classes_count=train_generator.num_classes,\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "X6hOh0gu_7wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_time = True"
      ],
      "metadata": {
        "id": "AcYA27hxAAU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def approximate_catboost_gradients(X, logits, epsilon=1e-5):\n",
        "    gradients = np.zeros((batch_size, latent_dim, train_generator.num_classes))\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        for j in range(latent_dim):\n",
        "            perturbed_data = np.copy(X[i, :])\n",
        "            perturbed_data[j] += epsilon\n",
        "            perturbed_predictions = cls_model.predict(perturbed_data, prediction_type=\"Probability\", verbose=0)\n",
        "\n",
        "            gradients[i, j, :] = (perturbed_predictions - logits[i, :]) / epsilon\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "Nf2zPebq6YVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_model_on_subset(X, y):\n",
        "    global first_time\n",
        "\n",
        "    y_argmax = np.argmax(y, axis=1)\n",
        "\n",
        "    if first_time:\n",
        "      cls_model.fit(X, y_argmax)\n",
        "      first_time = False\n",
        "    else:\n",
        "      cls_model.fit(X, y_argmax, init_model=cls_model)\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    grads = approximate_catboost_gradients(X, logits)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value, grads"
      ],
      "metadata": {
        "id": "w4HkaMXTADGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_model_on_subset(X, y):\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "DKvybGAgANGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(X, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        latent_vector = model(X, training=True)\n",
        "\n",
        "    np_latent_vector = latent_vector.numpy()\n",
        "\n",
        "    logits, loss_value, cls_grads = train_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "    grads = tape.gradient(tf.cast(latent_vector, tf.double), model.trainable_weights, output_gradients=tf.convert_to_tensor(cls_grads, dtype=tf.double))\n",
        "\n",
        "    print(\"--------------------------\")\n",
        "    print(grads)\n",
        "    print(\"--------------------------\")\n",
        "\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "xXcVWOjwAPNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(X, y):\n",
        "    latent_vector = model(X, training=False)\n",
        "\n",
        "    np_latent_vector = latent_vector.numpy()\n",
        "\n",
        "    logits, loss_value = test_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "    val_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "w8DgacI2APec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsSSXeM1APbp",
        "outputId": "92d60416-f4b8-4e7e-b848-8d915f7efe97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "573"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []"
      ],
      "metadata": {
        "id": "AfyfJMQjAXuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss_value_for_epoch = None\n",
        "    val_loss_value_for_epoch = None\n",
        "    for step, (X_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        loss_value = train_step(X_batch_train, y_batch_train)\n",
        "\n",
        "        if step % 25 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(train_generator):\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at last step: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "            train_generator.on_epoch_end()\n",
        "            train_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    train_loss_history.append(train_loss_value_for_epoch)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_state()\n",
        "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
        "\n",
        "    train_accuracy_history.append(train_acc)\n",
        "\n",
        "    for step, (X_batch_val, y_batch_val) in enumerate(val_generator):\n",
        "        loss_value = test_step(X_batch_val, y_batch_val)\n",
        "\n",
        "        print(\n",
        "            f\"Validation loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "        )\n",
        "        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(val_generator):\n",
        "            val_generator.on_epoch_end()\n",
        "            val_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    val_loss_history.append(val_loss_value_for_epoch)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
        "    print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    val_accuracy_history.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nUzYbfU4AYDD",
        "outputId": "29112822-dfa0-4d5d-836a-20a2c26d5c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "--------------------------\n",
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "--------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No gradients provided for any variable: (['dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_7/kernel:0' shape=(2048, 512) dtype=float32, numpy=\narray([[-0.02552587,  0.01976389,  0.00809691, ...,  0.03406109,\n         0.02645925, -0.02544805],\n       [ 0.01299418, -0.0170031 , -0.03648469, ..., -0.01851693,\n         0.00290418,  0.02563923],\n       [ 0.04660729,  0.03434531, -0.03094377, ..., -0.04604105,\n         0.0275844 , -0.02141793],\n       ...,\n       [-0.04295367,  0.04068042, -0.04646092, ..., -0.00765682,\n        -0.0314377 ,  0.01016685],\n       [ 0.02378953, -0.03602906,  0.03337136, ..., -0.00417072,\n         0.02366   , -0.03997187],\n       [-0.0358343 , -0.03580118,  0.03382479, ..., -0.01303279,\n        -0.01497388, -0.04230733]], dtype=float32)>), (None, <tf.Variable 'dense_7/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_8/kernel:0' shape=(512, 256) dtype=float32, numpy=\narray([[ 0.02086265,  0.02332706, -0.06051782, ..., -0.05598741,\n         0.01838972,  0.05659715],\n       [-0.01310954,  0.0113893 , -0.06242167, ..., -0.04633114,\n         0.0746144 , -0.06857862],\n       [ 0.0664496 ,  0.00569495,  0.03757099, ...,  0.04487247,\n        -0.04233567, -0.0761048 ],\n       ...,\n       [-0.00831854,  0.02527932,  0.0285708 , ..., -0.03707599,\n        -0.05552133,  0.08596311],\n       [-0.04944529,  0.00294258, -0.02537289, ..., -0.06527176,\n         0.01741371, -0.01357797],\n       [-0.06996644,  0.03349697,  0.00422893, ..., -0.0860838 ,\n        -0.08703879,  0.04375622]], dtype=float32)>), (None, <tf.Variable 'dense_8/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_9/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[ 0.12033081, -0.05373654,  0.02663392, ..., -0.08796406,\n        -0.11885619, -0.08449531],\n       [ 0.06886703,  0.00450161,  0.10414788, ...,  0.0632323 ,\n         0.0042277 ,  0.09126043],\n       [-0.07957706,  0.01847595, -0.09642223, ..., -0.02962992,\n         0.09307149,  0.02513412],\n       ...,\n       [ 0.12343422, -0.08318385,  0.02477929, ...,  0.05406696,\n         0.00162023,  0.06306708],\n       [ 0.04871216,  0.03561947, -0.02371559, ..., -0.08433878,\n         0.08569053, -0.00358224],\n       [ 0.04788455, -0.08304456,  0.01552042, ...,  0.09460297,\n         0.04286188, -0.03483871]], dtype=float32)>), (None, <tf.Variable 'dense_9/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_10/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[-0.0827126 , -0.00493185,  0.03162396, ..., -0.00914375,\n         0.11447881, -0.16280256],\n       [-0.05804452, -0.15653934,  0.1296056 , ...,  0.11437871,\n         0.06764215,  0.09279843],\n       [ 0.08211516, -0.14055943, -0.05816016, ..., -0.05678428,\n        -0.17089397, -0.11608734],\n       ...,\n       [-0.00032622,  0.11405404,  0.1460417 , ...,  0.13476403,\n        -0.06605616,  0.04090284],\n       [ 0.16159762,  0.11361893,  0.16373132, ..., -0.03763781,\n        -0.01080169,  0.05053259],\n       [ 0.13582702, -0.1401776 , -0.13075566, ...,  0.10292341,\n         0.10859536, -0.09068341]], dtype=float32)>), (None, <tf.Variable 'dense_10/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_11/kernel:0' shape=(64, 128) dtype=float32, numpy=\narray([[ 1.19467840e-01,  1.63714275e-01,  6.36813194e-02, ...,\n        -1.76699430e-01,  1.61476806e-01, -1.03466168e-01],\n       [ 1.39251053e-02,  9.07806009e-02,  2.31519341e-02, ...,\n        -1.53709471e-01,  1.20959803e-01, -2.39265561e-02],\n       [ 1.68845057e-04,  1.20320454e-01, -3.08394432e-02, ...,\n         1.16242483e-01, -2.36216187e-03,  7.05252886e-02],\n       ...,\n       [-8.13993067e-02, -8.63014460e-02, -3.48975062e-02, ...,\n        -6.61958754e-03, -4.67094183e-02, -1.25973463e-01],\n       [ 1.40289202e-01,  6.79326206e-02, -8.96530896e-02, ...,\n        -6.73756525e-02, -1.54251695e-01,  1.30747691e-01],\n       [ 1.17905721e-01, -4.96918112e-02, -3.33941728e-02, ...,\n        -6.83720037e-02,  1.44255176e-01,  1.33289203e-01]], dtype=float32)>), (None, <tf.Variable 'dense_11/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_12/kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.07316715, -0.07420337,  0.07920349, ...,  0.04626146,\n         0.03033251, -0.01651298],\n       [-0.06455964, -0.02142083, -0.01956726, ..., -0.07023789,\n        -0.02418061, -0.09071268],\n       [ 0.05767132, -0.01610187, -0.05593782, ...,  0.03817029,\n         0.00636166,  0.07514334],\n       ...,\n       [ 0.08139689, -0.00149868,  0.03027511, ...,  0.03220664,\n         0.09020318, -0.00463685],\n       [ 0.05548677, -0.00556727, -0.05179569, ..., -0.01093215,\n         0.07664791, -0.07656709],\n       [-0.04654097, -0.01588332, -0.06051368, ...,  0.00709353,\n         0.04597248, -0.03890476]], dtype=float32)>), (None, <tf.Variable 'dense_12/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_13/kernel:0' shape=(512, 512) dtype=float32, numpy=\narray([[-0.03890856, -0.06441525,  0.00910039, ...,  0.05507798,\n         0.0526515 ,  0.07296643],\n       [ 0.07517453, -0.06645506,  0.01443084, ..., -0.06985577,\n        -0.03545515,  0.03019156],\n       [-0.02395447,  0.00168879,  0.03911324, ..., -0.06466243,\n        -0.03843629,  0.06649198],\n       ...,\n       [ 0.0746079 , -0.0573164 , -0.05750555, ...,  0.02206637,\n        -0.02558087,  0.02995131],\n       [ 0.07125003, -0.0144668 ,  0.06967948, ...,  0.03576095,\n         0.00583076,  0.0618074 ],\n       [ 0.0687181 , -0.00644151,  0.01994875, ...,  0.0094413 ,\n        -0.05031223, -0.01693055]], dtype=float32)>), (None, <tf.Variable 'dense_13/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>)).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-87669c519f31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss_value_for_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-965a5b60c2cd>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         )\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36maggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_reduce_sum_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     def apply_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\u001b[0m in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mfiltered_grads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfiltered_grads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;34mf\"No gradients provided for any variable: {variable}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;34mf\"Provided `grads_and_vars` is {grads_and_vars}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense_7/kernel:0', 'dense_7/bias:0', 'dense_8/kernel:0', 'dense_8/bias:0', 'dense_9/kernel:0', 'dense_9/bias:0', 'dense_10/kernel:0', 'dense_10/bias:0', 'dense_11/kernel:0', 'dense_11/bias:0', 'dense_12/kernel:0', 'dense_12/bias:0', 'dense_13/kernel:0', 'dense_13/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense_7/kernel:0' shape=(2048, 512) dtype=float32, numpy=\narray([[-0.02552587,  0.01976389,  0.00809691, ...,  0.03406109,\n         0.02645925, -0.02544805],\n       [ 0.01299418, -0.0170031 , -0.03648469, ..., -0.01851693,\n         0.00290418,  0.02563923],\n       [ 0.04660729,  0.03434531, -0.03094377, ..., -0.04604105,\n         0.0275844 , -0.02141793],\n       ...,\n       [-0.04295367,  0.04068042, -0.04646092, ..., -0.00765682,\n        -0.0314377 ,  0.01016685],\n       [ 0.02378953, -0.03602906,  0.03337136, ..., -0.00417072,\n         0.02366   , -0.03997187],\n       [-0.0358343 , -0.03580118,  0.03382479, ..., -0.01303279,\n        -0.01497388, -0.04230733]], dtype=float32)>), (None, <tf.Variable 'dense_7/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_8/kernel:0' shape=(512, 256) dtype=float32, numpy=\narray([[ 0.02086265,  0.02332706, -0.06051782, ..., -0.05598741,\n         0.01838972,  0.05659715],\n       [-0.01310954,  0.0113893 , -0.06242167, ..., -0.04633114,\n         0.0746144 , -0.06857862],\n       [ 0.0664496 ,  0.00569495,  0.03757099, ...,  0.04487247,\n        -0.04233567, -0.0761048 ],\n       ...,\n       [-0.00831854,  0.02527932,  0.0285708 , ..., -0.03707599,\n        -0.05552133,  0.08596311],\n       [-0.04944529,  0.00294258, -0.02537289, ..., -0.06527176,\n         0.01741371, -0.01357797],\n       [-0.06996644,  0.03349697,  0.00422893, ..., -0.0860838 ,\n        -0.08703879,  0.04375622]], dtype=float32)>), (None, <tf.Variable 'dense_8/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_9/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[ 0.12033081, -0.05373654,  0.02663392, ..., -0.08796406,\n        -0.11885619, -0.08449531],\n       [ 0.06886703,  0.00450161,  0.10414788, ...,  0.0632323 ,\n         0.0042277 ,  0.09126043],\n       [-0.07957706,  0.01847595, -0.09642223, ..., -0.02962992,\n         0.09307149,  0.02513412],\n       ...,\n       [ 0.12343422, -0.08318385,  0.02477929, ...,  0.05406696,\n         0.00162023,  0.06306708],\n       [ 0.04871216,  0.03561947, -0.02371559, ..., -0.08433878,\n         0.08569053, -0.00358224],\n       [ 0.04788455, -0.08304456,  0.01552042, ...,  0.09460297,\n         0.04286188, -0.03483871]], dtype=float32)>), (None, <tf.Variable 'dense_9/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_10/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[-0.0827126 , -0.00493185,  0.03162396, ..., -0.00914375,\n         0.11447881, -0.16280256],\n       [-0.05804452, -0.15653934,  0.1296056 , ...,  0.11437871,\n         0.06764215,  0.09279843],\n       [ 0.08211516, -0.14055943, -0.05816016, ..., -0.05678428,\n        -0.17089397, -0.11608734],\n       ...,\n       [-0.00032622,  0.11405404,  0.1460417 , ...,  0.13476403,\n        -0.06605616,  0.04090284],\n       [ 0.16159762,  0.11361893,  0.16373132, ..., -0.03763781,\n        -0.01080169,  0.05053259],\n       [ 0.13582702, -0.1401776 , -0.13075566, ...,  0.10292341,\n         0.10859536, -0.09068341]], dtype=float32)>), (None, <tf.Variable 'dense_10/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_11/kernel:0' shape=(64, 128) dtype=float32, numpy=\narray([[ 1.19467840e-01,  1.63714275e-01,  6.36813194e-02, ...,\n        -1.76699430e-01,  1.61476806e-01, -1.03466168e-01],\n       [ 1.39251053e-02,  9.07806009e-02,  2.31519341e-02, ...,\n        -1.53709471e-01,  1.20959803e-01, -2.39265561e-02],\n       [ 1.68845057e-04,  1.20320454e-01, -3.08394432e-02, ...,\n         1.16242483e-01, -2.36216187e-03,  7.05252886e-02],\n       ...,\n       [-8.13993067e-02, -8.63014460e-02, -3.48975062e-02, ...,\n        -6.61958754e-03, -4.67094183e-02, -1.25973463e-01],\n       [ 1.40289202e-01,  6.79326206e-02, -8.96530896e-02, ...,\n        -6.73756525e-02, -1.54251695e-01,  1.30747691e-01],\n       [ 1.17905721e-01, -4.96918112e-02, -3.33941728e-02, ...,\n        -6.83720037e-02,  1.44255176e-01,  1.33289203e-01]], dtype=float32)>), (None, <tf.Variable 'dense_11/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_12/kernel:0' shape=(128, 512) dtype=float32, numpy=\narray([[-0.07316715, -0.07420337,  0.07920349, ...,  0.04626146,\n         0.03033251, -0.01651298],\n       [-0.06455964, -0.02142083, -0.01956726, ..., -0.07023789,\n        -0.02418061, -0.09071268],\n       [ 0.05767132, -0.01610187, -0.05593782, ...,  0.03817029,\n         0.00636166,  0.07514334],\n       ...,\n       [ 0.08139689, -0.00149868,  0.03027511, ...,  0.03220664,\n         0.09020318, -0.00463685],\n       [ 0.05548677, -0.00556727, -0.05179569, ..., -0.01093215,\n         0.07664791, -0.07656709],\n       [-0.04654097, -0.01588332, -0.06051368, ...,  0.00709353,\n         0.04597248, -0.03890476]], dtype=float32)>), (None, <tf.Variable 'dense_12/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_13/kernel:0' shape=(512, 512) dtype=float32, numpy=\narray([[-0.03890856, -0.06441525,  0.00910039, ...,  0.05507798,\n         0.0526515 ,  0.07296643],\n       [ 0.07517453, -0.06645506,  0.01443084, ..., -0.06985577,\n        -0.03545515,  0.03019156],\n       [-0.02395447,  0.00168879,  0.03911324, ..., -0.06466243,\n        -0.03843629,  0.06649198],\n       ...,\n       [ 0.0746079 , -0.0573164 , -0.05750555, ...,  0.02206637,\n        -0.02558087,  0.02995131],\n       [ 0.07125003, -0.0144668 ,  0.06967948, ...,  0.03576095,\n         0.00583076,  0.0618074 ],\n       [ 0.0687181 , -0.00644151,  0.01994875, ...,  0.0094413 ,\n        -0.05031223, -0.01693055]], dtype=float32)>), (None, <tf.Variable 'dense_13/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>))."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "7_S6yoqdULHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ALTINCI DENEME (Burada da yukarıdaki modelin aynısını kullandım ve gradient'leri genetik algoritma kullanarak üretmeye çalıştım ki böylece rastgele modelin işine yaramayacak değil, loss fonksiyonun da yardımıyla doğru gradient'leri bulmaya çalıştım ama bu modelin çalışma zamanı çok uzun oldu o yüzden bundan da vazgeçtim.)"
      ],
      "metadata": {
        "id": "AKJOHu2-T6KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "Elv8nxBLUHzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "latent_vector = Dense(latent_dim, activation='linear')(x)"
      ],
      "metadata": {
        "id": "0sj09PCTUM4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=latent_vector)"
      ],
      "metadata": {
        "id": "zrQozk1qUNk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "rQ-QAab5UOE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "loss_fn = CategoricalCrossentropy()\n",
        "train_acc_metric = CategoricalAccuracy()\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "LTpXQPdwUORC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier, Pool, sum_models"
      ],
      "metadata": {
        "id": "_lLTPcXeUOhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_model = CatBoostClassifier(\n",
        "    iterations=cls_epochs,\n",
        "    learning_rate=cls_learning_rate,\n",
        "    depth=cls_depth,\n",
        "    classes_count=train_generator.num_classes,\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "UB2rjaW6UOsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_time = True"
      ],
      "metadata": {
        "id": "6iPtrdzbUO16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_model_on_subset(X, y):\n",
        "    global first_time\n",
        "\n",
        "    y_argmax = np.argmax(y, axis=1)\n",
        "\n",
        "    if first_time:\n",
        "      cls_model.fit(X, y_argmax)\n",
        "      first_time = False\n",
        "    else:\n",
        "      cls_model.fit(X, y_argmax, init_model=cls_model)\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "pjJ_K2oEUPGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_model_on_subset(X, y):\n",
        "    logits = cls_model.predict(X, prediction_type=\"Probability\", verbose=0)\n",
        "    loss_value = loss_fn(y, logits)\n",
        "    return logits, loss_value"
      ],
      "metadata": {
        "id": "jRtXVi62UPRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(X, y):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        latent_vector = model(X, training=True)\n",
        "\n",
        "    grads = tape.gradient(tf.constant(1.245, dtype=tf.double), model.trainable_weights)\n",
        "\n",
        "    print(grads)\n",
        "    np_latent_vector = latent_vector.numpy()\n",
        "\n",
        "    logits, loss_value = train_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "    print(loss_value)\n",
        "\n",
        "\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "hCR3rXFlUPhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(X, y):\n",
        "    latent_vector = model(X, training=False)\n",
        "\n",
        "    np_latent_vector = latent_vector.numpy()\n",
        "\n",
        "    logits, loss_value = test_model_on_subset(np_latent_vector, y)\n",
        "\n",
        "    val_acc_metric.update_state(y, logits)\n",
        "\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "edqUfTGoUu35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDTArr8iUvCr",
        "outputId": "80fcdc00-891a-44c4-f401-db880edd96a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []"
      ],
      "metadata": {
        "id": "Q52R5T1XUvMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss_value_for_epoch = None\n",
        "    val_loss_value_for_epoch = None\n",
        "    for step, (X_batch_train, y_batch_train) in enumerate(train_generator):\n",
        "        loss_value = train_step(X_batch_train, y_batch_train)\n",
        "\n",
        "        if step % 25 == 0:\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(train_generator):\n",
        "            print(\n",
        "                f\"Training loss (for 1 batch) at last step: {float(loss_value):.4f}\"\n",
        "            )\n",
        "            print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "            train_generator.on_epoch_end()\n",
        "            train_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    train_loss_history.append(train_loss_value_for_epoch)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_state()\n",
        "    print(f\"Training acc over epoch: {float(train_acc):.4f}\")\n",
        "\n",
        "    train_accuracy_history.append(train_acc)\n",
        "\n",
        "    for step, (X_batch_val, y_batch_val) in enumerate(val_generator):\n",
        "        loss_value = test_step(X_batch_val, y_batch_val)\n",
        "\n",
        "        print(\n",
        "            f\"Validation loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
        "        )\n",
        "        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n",
        "\n",
        "        if step + 1 >= len(val_generator):\n",
        "            val_generator.on_epoch_end()\n",
        "            val_loss_value_for_epoch = loss_value\n",
        "            break\n",
        "\n",
        "    val_loss_history.append(val_loss_value_for_epoch)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(f\"Validation acc: {float(val_acc):.4f}\")\n",
        "    print(f\"Time taken: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    val_accuracy_history.append(val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UcMOqDqJUvUR",
        "outputId": "fe1322ec-10bf-4f21-a3a8-04a5d617f569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
            "tf.Tensor(1.142583981184272, shape=(), dtype=float64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense/kernel:0' shape=(2048, 256) dtype=float32, numpy=\narray([[ 0.03704067, -0.04128283, -0.00222154, ...,  0.0268454 ,\n         0.00362879, -0.02648962],\n       [-0.01117669,  0.00675936, -0.02827097, ..., -0.02385554,\n         0.04811696, -0.03036369],\n       [ 0.00600742, -0.00497658, -0.03225531, ...,  0.03740581,\n        -0.00505031,  0.02738799],\n       ...,\n       [ 0.03091831,  0.03147932,  0.02971395, ..., -0.01843922,\n         0.04672048, -0.00996908],\n       [-0.03861097,  0.04138049, -0.03334405, ...,  0.03670251,\n         0.00513542, -0.01182823],\n       [ 0.0354064 ,  0.01039723,  0.00434591, ..., -0.04140163,\n         0.04629208, -0.00272114]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[ 0.09868616,  0.02090028,  0.10834342, ..., -0.05316064,\n        -0.03729948, -0.06392297],\n       [-0.00454766,  0.03620464, -0.09945875, ..., -0.0899289 ,\n         0.04865757,  0.09898916],\n       [-0.04273513,  0.11422482, -0.04052129, ...,  0.10364142,\n         0.05802652, -0.00452921],\n       ...,\n       [-0.10334969,  0.02913392,  0.04204586, ..., -0.0582211 ,\n         0.11484385,  0.02415788],\n       [ 0.04212052,  0.07997745,  0.06725892, ..., -0.02055904,\n        -0.04740939,  0.00669   ],\n       [-0.1056239 ,  0.04824921,  0.10059837, ..., -0.0962429 ,\n         0.05294895,  0.06458238]], dtype=float32)>), (None, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[ 0.00182344, -0.15329373,  0.12935959, ..., -0.03159286,\n        -0.02116181, -0.05528406],\n       [-0.10751349,  0.01484428,  0.01262778, ...,  0.08583526,\n         0.06006001,  0.01823007],\n       [-0.08235418, -0.08617382, -0.05680451, ..., -0.11283267,\n         0.17633854, -0.12860082],\n       ...,\n       [-0.11862399,  0.00304562, -0.12810746, ...,  0.09977318,\n        -0.07013467,  0.03628439],\n       [ 0.09176932,  0.10514228,  0.08472474, ...,  0.15930568,\n         0.17320685, -0.09296414],\n       [-0.15046029, -0.07121127, -0.15013023, ...,  0.10848029,\n         0.03103015,  0.11432017]], dtype=float32)>), (None, <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_3/kernel:0' shape=(64, 32) dtype=float32, numpy=\narray([[-0.16858739,  0.08123368,  0.02419829, ..., -0.04314566,\n         0.1644674 ,  0.05769974],\n       [-0.07875836,  0.00739843,  0.00158936, ...,  0.16179079,\n         0.12700605,  0.16295666],\n       [-0.04448438,  0.2117536 , -0.2029388 , ...,  0.04877377,\n         0.05099404,  0.24949098],\n       ...,\n       [ 0.07989258, -0.13547415,  0.13308513, ...,  0.0433045 ,\n        -0.10334164,  0.22102058],\n       [-0.24810052, -0.17671216,  0.1463629 , ..., -0.05961651,\n        -0.21792138,  0.01246351],\n       [ 0.0080592 , -0.09515959,  0.08885378, ...,  0.24666125,\n         0.08073682, -0.00228834]], dtype=float32)>), (None, <tf.Variable 'dense_3/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_4/kernel:0' shape=(32, 64) dtype=float32, numpy=\narray([[-0.01006073,  0.24364138,  0.06946594, ...,  0.0745818 ,\n         0.14220446,  0.11743045],\n       [ 0.17362922, -0.22238576,  0.24665111, ..., -0.05739981,\n         0.04539376,  0.03842151],\n       [-0.15944964, -0.14057666, -0.21008837, ...,  0.06241959,\n         0.24666512,  0.07122797],\n       ...,\n       [ 0.17006898, -0.0602501 , -0.1807813 , ...,  0.09644538,\n        -0.15802228, -0.11734849],\n       [-0.00726622,  0.13953704, -0.12982297, ...,  0.00959629,\n         0.09450513,  0.06609756],\n       [ 0.00235665, -0.07981902, -0.01526612, ...,  0.1445871 ,\n        -0.04458249,  0.16219491]], dtype=float32)>), (None, <tf.Variable 'dense_4/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_5/kernel:0' shape=(64, 128) dtype=float32, numpy=\narray([[-0.13263217, -0.09940056,  0.0736668 , ..., -0.1122655 ,\n         0.07391427,  0.03507359],\n       [ 0.0909947 , -0.17575169, -0.12459679, ..., -0.04499909,\n        -0.02523011, -0.16205366],\n       [-0.03676103, -0.16844717, -0.16527899, ...,  0.16029583,\n        -0.01469357,  0.15039293],\n       ...,\n       [-0.01408826,  0.102654  , -0.05021046, ...,  0.12953196,\n        -0.14141671,  0.11598344],\n       [-0.09540293, -0.1124244 , -0.03796348, ..., -0.01102258,\n         0.0061606 ,  0.01811214],\n       [-0.10587857,  0.07138985,  0.07572667, ..., -0.01615307,\n         0.14918493,  0.14042176]], dtype=float32)>), (None, <tf.Variable 'dense_5/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_6/kernel:0' shape=(128, 256) dtype=float32, numpy=\narray([[ 0.01696736,  0.06762162,  0.00025019, ..., -0.11983863,\n         0.0346109 , -0.08311319],\n       [-0.09503567,  0.04800576,  0.03974798, ...,  0.01172611,\n        -0.05547553, -0.04247323],\n       [ 0.09165752, -0.09945148, -0.11690322, ..., -0.01182297,\n         0.09312171,  0.01238137],\n       ...,\n       [ 0.03302413, -0.02855989, -0.04005259, ..., -0.02109689,\n         0.09802872,  0.11190993],\n       [ 0.11569095,  0.06858546, -0.06209403, ...,  0.02437299,\n        -0.02716544, -0.09207129],\n       [-0.05515972,  0.00029987,  0.12299368, ...,  0.0844565 ,\n         0.04699221,  0.03940752]], dtype=float32)>), (None, <tf.Variable 'dense_6/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_7/kernel:0' shape=(256, 512) dtype=float32, numpy=\narray([[ 0.05672441,  0.00612107,  0.05767391, ..., -0.001086  ,\n         0.02145968,  0.07873432],\n       [ 0.02131139,  0.06138682, -0.07519773, ..., -0.0249499 ,\n        -0.0808433 ,  0.07953013],\n       [ 0.01268833,  0.00984879,  0.05017435, ...,  0.01370302,\n        -0.06959224, -0.02382133],\n       ...,\n       [ 0.05367144,  0.08416612, -0.01806016, ...,  0.03115758,\n        -0.07844005,  0.01202892],\n       [-0.07751408, -0.08392219,  0.04429773, ..., -0.05037345,\n        -0.07807031, -0.03547011],\n       [-0.03553797, -0.02699626, -0.03334207, ..., -0.07117159,\n        -0.03829648, -0.0877365 ]], dtype=float32)>), (None, <tf.Variable 'dense_7/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>)).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-87669c519f31>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss_value_for_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d867b45b8986>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         )\n\u001b[1;32m   1221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_gradients_aggregation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexperimental_aggregate_gradients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m             \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py\u001b[0m in \u001b[0;36maggregate_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_reduce_sum_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m     def apply_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\u001b[0m in \u001b[0;36mall_reduce_sum_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \"\"\"\n\u001b[1;32m     32\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mfiltered_grads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_empty_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfiltered_grads_and_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy_supports_no_merge_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/utils.py\u001b[0m in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;34mf\"No gradients provided for any variable: {variable}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;34mf\"Provided `grads_and_vars` is {grads_and_vars}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'dense_6/kernel:0', 'dense_6/bias:0', 'dense_7/kernel:0', 'dense_7/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense/kernel:0' shape=(2048, 256) dtype=float32, numpy=\narray([[ 0.03704067, -0.04128283, -0.00222154, ...,  0.0268454 ,\n         0.00362879, -0.02648962],\n       [-0.01117669,  0.00675936, -0.02827097, ..., -0.02385554,\n         0.04811696, -0.03036369],\n       [ 0.00600742, -0.00497658, -0.03225531, ...,  0.03740581,\n        -0.00505031,  0.02738799],\n       ...,\n       [ 0.03091831,  0.03147932,  0.02971395, ..., -0.01843922,\n         0.04672048, -0.00996908],\n       [-0.03861097,  0.04138049, -0.03334405, ...,  0.03670251,\n         0.00513542, -0.01182823],\n       [ 0.0354064 ,  0.01039723,  0.00434591, ..., -0.04140163,\n         0.04629208, -0.00272114]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_1/kernel:0' shape=(256, 128) dtype=float32, numpy=\narray([[ 0.09868616,  0.02090028,  0.10834342, ..., -0.05316064,\n        -0.03729948, -0.06392297],\n       [-0.00454766,  0.03620464, -0.09945875, ..., -0.0899289 ,\n         0.04865757,  0.09898916],\n       [-0.04273513,  0.11422482, -0.04052129, ...,  0.10364142,\n         0.05802652, -0.00452921],\n       ...,\n       [-0.10334969,  0.02913392,  0.04204586, ..., -0.0582211 ,\n         0.11484385,  0.02415788],\n       [ 0.04212052,  0.07997745,  0.06725892, ..., -0.02055904,\n        -0.04740939,  0.00669   ],\n       [-0.1056239 ,  0.04824921,  0.10059837, ..., -0.0962429 ,\n         0.05294895,  0.06458238]], dtype=float32)>), (None, <tf.Variable 'dense_1/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_2/kernel:0' shape=(128, 64) dtype=float32, numpy=\narray([[ 0.00182344, -0.15329373,  0.12935959, ..., -0.03159286,\n        -0.02116181, -0.05528406],\n       [-0.10751349,  0.01484428,  0.01262778, ...,  0.08583526,\n         0.06006001,  0.01823007],\n       [-0.08235418, -0.08617382, -0.05680451, ..., -0.11283267,\n         0.17633854, -0.12860082],\n       ...,\n       [-0.11862399,  0.00304562, -0.12810746, ...,  0.09977318,\n        -0.07013467,  0.03628439],\n       [ 0.09176932,  0.10514228,  0.08472474, ...,  0.15930568,\n         0.17320685, -0.09296414],\n       [-0.15046029, -0.07121127, -0.15013023, ...,  0.10848029,\n         0.03103015,  0.11432017]], dtype=float32)>), (None, <tf.Variable 'dense_2/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_3/kernel:0' shape=(64, 32) dtype=float32, numpy=\narray([[-0.16858739,  0.08123368,  0.02419829, ..., -0.04314566,\n         0.1644674 ,  0.05769974],\n       [-0.07875836,  0.00739843,  0.00158936, ...,  0.16179079,\n         0.12700605,  0.16295666],\n       [-0.04448438,  0.2117536 , -0.2029388 , ...,  0.04877377,\n         0.05099404,  0.24949098],\n       ...,\n       [ 0.07989258, -0.13547415,  0.13308513, ...,  0.0433045 ,\n        -0.10334164,  0.22102058],\n       [-0.24810052, -0.17671216,  0.1463629 , ..., -0.05961651,\n        -0.21792138,  0.01246351],\n       [ 0.0080592 , -0.09515959,  0.08885378, ...,  0.24666125,\n         0.08073682, -0.00228834]], dtype=float32)>), (None, <tf.Variable 'dense_3/bias:0' shape=(32,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_4/kernel:0' shape=(32, 64) dtype=float32, numpy=\narray([[-0.01006073,  0.24364138,  0.06946594, ...,  0.0745818 ,\n         0.14220446,  0.11743045],\n       [ 0.17362922, -0.22238576,  0.24665111, ..., -0.05739981,\n         0.04539376,  0.03842151],\n       [-0.15944964, -0.14057666, -0.21008837, ...,  0.06241959,\n         0.24666512,  0.07122797],\n       ...,\n       [ 0.17006898, -0.0602501 , -0.1807813 , ...,  0.09644538,\n        -0.15802228, -0.11734849],\n       [-0.00726622,  0.13953704, -0.12982297, ...,  0.00959629,\n         0.09450513,  0.06609756],\n       [ 0.00235665, -0.07981902, -0.01526612, ...,  0.1445871 ,\n        -0.04458249,  0.16219491]], dtype=float32)>), (None, <tf.Variable 'dense_4/bias:0' shape=(64,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_5/kernel:0' shape=(64, 128) dtype=float32, numpy=\narray([[-0.13263217, -0.09940056,  0.0736668 , ..., -0.1122655 ,\n         0.07391427,  0.03507359],\n       [ 0.0909947 , -0.17575169, -0.12459679, ..., -0.04499909,\n        -0.02523011, -0.16205366],\n       [-0.03676103, -0.16844717, -0.16527899, ...,  0.16029583,\n        -0.01469357,  0.15039293],\n       ...,\n       [-0.01408826,  0.102654  , -0.05021046, ...,  0.12953196,\n        -0.14141671,  0.11598344],\n       [-0.09540293, -0.1124244 , -0.03796348, ..., -0.01102258,\n         0.0061606 ,  0.01811214],\n       [-0.10587857,  0.07138985,  0.07572667, ..., -0.01615307,\n         0.14918493,  0.14042176]], dtype=float32)>), (None, <tf.Variable 'dense_5/bias:0' shape=(128,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>), (None, <tf.Variable 'dense_6/kernel:0' shape=(128, 256) dtype=float32, numpy=\narray([[ 0.01696736,  0.06762162,  0.00025019, ..., -0.11983863,\n         0.0346109 , -0.08311319],\n       [-0.09503567,  0.04800576,  0.03974798, ...,  0.01172611,\n        -0.05547553, -0.04247323],\n       [ 0.09165752, -0.09945148, -0.11690322, ..., -0.01182297,\n         0.09312171,  0.01238137],\n       ...,\n       [ 0.03302413, -0.02855989, -0.04005259, ..., -0.02109689,\n         0.09802872,  0.11190993],\n       [ 0.11569095,  0.06858546, -0.06209403, ...,  0.02437299,\n        -0.02716544, -0.09207129],\n       [-0.05515972,  0.00029987,  0.12299368, ...,  0.0844565 ,\n         0.04699221,  0.03940752]], dtype=float32)>), (None, <tf.Variable 'dense_6/bias:0' shape=(256,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0.], dtype=float32)>), (None, <tf.Variable 'dense_7/kernel:0' shape=(256, 512) dtype=float32, numpy=\narray([[ 0.05672441,  0.00612107,  0.05767391, ..., -0.001086  ,\n         0.02145968,  0.07873432],\n       [ 0.02131139,  0.06138682, -0.07519773, ..., -0.0249499 ,\n        -0.0808433 ,  0.07953013],\n       [ 0.01268833,  0.00984879,  0.05017435, ...,  0.01370302,\n        -0.06959224, -0.02382133],\n       ...,\n       [ 0.05367144,  0.08416612, -0.01806016, ...,  0.03115758,\n        -0.07844005,  0.01202892],\n       [-0.07751408, -0.08392219,  0.04429773, ..., -0.05037345,\n        -0.07807031, -0.03547011],\n       [-0.03553797, -0.02699626, -0.03334207, ..., -0.07117159,\n        -0.03829648, -0.0877365 ]], dtype=float32)>), (None, <tf.Variable 'dense_7/bias:0' shape=(512,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>))."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "tjqnKd_IZpZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YEDİNCİ DENEME - ÇOK UZUN BİR UĞRAŞ SONUCU KLASİK ÇÖZÜME DÖNÜŞ (Toplam 4 session yapıldı ve 26 farklı mimari ve hiperparametre denendi.)"
      ],
      "metadata": {
        "id": "Qdzyrs51ZpDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner.tuners import Hyperband"
      ],
      "metadata": {
        "id": "um7j3N5cbajK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
        "        dense_units = hp.Choice('dense_units', values=[256, 512, 1024, 2048])\n",
        "        print(\"for \", i)\n",
        "        print(dense_units)\n",
        "\n",
        "        if hp.Boolean(\"regularizetion\"):\n",
        "            regularizetion_rate = hp.Choice('regularizetion_rate', values=[1e-3, 1e-4, 1e-5])\n",
        "            print(regularizetion_rate)\n",
        "            model.add(Dense(dense_units, activation=\"relu\", kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10)))\n",
        "        else:\n",
        "            model.add(Dense(dense_units, activation=\"relu\"))\n",
        "\n",
        "        if hp.Boolean(\"dropout\"):\n",
        "            dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "            print(dropout_rate)\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(train_generator.num_classes, activation=\"softmax\"))\n",
        "\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4])\n",
        "    optimizer = hp.Choice('optimizer', values=['sgd', 'adam'])\n",
        "\n",
        "    if optimizer == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
        "    elif optimizer == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise\n",
        "    print(optimizer)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TWuZhlwfbVPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = Hyperband(build_model,\n",
        "                  objective='val_accuracy',\n",
        "                  max_epochs=10,\n",
        "                  factor=3,\n",
        "                  directory='models',\n",
        "                  project_name='deeplearningfinal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZjNzCvmZnh8",
        "outputId": "a434b5bf-e4a5-4d47-a63d-e2089983e4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from models/deeplearningfinal/tuner0.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "rEzsx0HHgyoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(train_generator, epochs=50, validation_data=val_generator, callbacks=[stop_early])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKvukgeXg7TO",
        "outputId": "eb0e4afe-4ac7-4898-fe44-d54537246b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9 Complete [00h 56m 58s]\n",
            "val_accuracy: 0.48645833134651184\n",
            "\n",
            "Best val_accuracy So Far: 0.48645833134651184\n",
            "Total elapsed time: 06h 18m 28s\n",
            "\n",
            "Search: Running Trial #10\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "768               |512               |dense_units\n",
            "1                 |1                 |num_layers\n",
            "True              |False             |regularizetion\n",
            "False             |False             |dropout\n",
            "0.0001            |0.01              |learning_rate\n",
            "0                 |0.4               |dropout_rate\n",
            "0.0001            |0.0001            |regularizetion_rate\n",
            "sgd               |sgd               |optimizer\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "2                 |2                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "for  0\n",
            "768\n",
            "0.0001\n",
            "<keras.src.optimizers.sgd.SGD object at 0x78cfca1504c0>\n",
            "Epoch 1/2\n",
            " 88/108 [=======================>......] - ETA: 4:39 - loss: 7.3889 - accuracy: 0.1053"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "id": "LY0tioSKhb-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "history = hypermodel.fit(train_generator, epochs=50, validation_data=val_generator)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "cA0XkcWKhSk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hhypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "hhypermodel.fit(train_generator, epochs=50, validation_data=val_generator)"
      ],
      "metadata": {
        "id": "RVvPRn3thgGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "aBmsDy9GGXfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EN İYİ PARAMETRELERE GÖRE MODEL EĞİTİMİ (Yukarıdaki 1 deneme dışında hiçbiri çalışmadığı için klasik çözüme dönüş yaptım. En çok zamanı yukarıda harcadım(aralıklı olmak üzere yaklaşık 1.5 hafta), hiperparametre araştırması yaptıktan sonra bu kısma an itibariyle ayın 12'sinde başlıyorum.)"
      ],
      "metadata": {
        "id": "EWc77hlnGW0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "regularizetion_rate = 5e-4\n",
        "dropout_rate = 0.2\n",
        "learning_rate = 1e-3"
      ],
      "metadata": {
        "id": "YW46fqYiGKN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgnH7fC--vgl",
        "outputId": "e2073a4b-e828-4dd8-f91e-f2f81d77fe9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"resnet50v2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_preact_bn (Ba  (None, 56, 56, 64)           256       ['pool1_pool[0][0]']          \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_preact_relu (  (None, 56, 56, 64)           0         ['conv2_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4096      ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_conv[0][0]', \n",
            "                                                                     'conv2_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block2_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block3_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 28, 28, 64)           36864     ['conv2_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 28, 28, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 28, 28, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 28, 28, 256)          0         ['conv2_block2_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 28, 28, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_out (Add)      (None, 28, 28, 256)          0         ['max_pooling2d_6[0][0]',     \n",
            "                                                                     'conv2_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block1_preact_bn (Ba  (None, 28, 28, 256)          1024      ['conv2_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block1_preact_relu (  (None, 28, 28, 256)          0         ['conv3_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32768     ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_conv[0][0]', \n",
            "                                                                     'conv3_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block2_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block2_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block3_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block3_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_out (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block4_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block4_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 14, 14, 128)          147456    ['conv3_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 14, 14, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 14, 14, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 14, 14, 512)          0         ['conv3_block3_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 14, 14, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_out (Add)      (None, 14, 14, 512)          0         ['max_pooling2d_7[0][0]',     \n",
            "                                                                     'conv3_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block1_preact_bn (Ba  (None, 14, 14, 512)          2048      ['conv3_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block1_preact_relu (  (None, 14, 14, 512)          0         ['conv4_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131072    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_conv[0][0]', \n",
            "                                                                     'conv4_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block2_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block2_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block3_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block3_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block4_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block4_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block5_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block5_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block5_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block5_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block5_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block5_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block6_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block5_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block6_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block6_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block6_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block6_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 7, 7, 256)            589824    ['conv4_block6_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 7, 7, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 7, 7, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 7, 7, 1024)           0         ['conv4_block5_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 7, 7, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_out (Add)      (None, 7, 7, 1024)           0         ['max_pooling2d_8[0][0]',     \n",
            "                                                                     'conv4_block6_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block1_preact_bn (Ba  (None, 7, 7, 1024)           4096      ['conv4_block6_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block1_preact_relu (  (None, 7, 7, 1024)           0         ['conv5_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524288    ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_conv[0][0]', \n",
            "                                                                     'conv5_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block2_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block2_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block3_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block3_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " post_bn (BatchNormalizatio  (None, 7, 7, 2048)           8192      ['conv5_block3_out[0][0]']    \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " post_relu (Activation)      (None, 7, 7, 2048)           0         ['post_bn[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23564800 (89.89 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 23564800 (89.89 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "IRuD1cEKA8KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(512, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(256, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(128, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(128, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(64, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(64, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(64, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(32, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(32, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "x = Dense(32, kernel_regularizer=regularizers.L1L2(l1=regularizetion_rate, l2=regularizetion_rate * 10))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "preds = Dense(train_generator.num_classes, activation=\"softmax\")(x)"
      ],
      "metadata": {
        "id": "ZZzxOQnQHFIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(inputs=base_model.input, outputs=preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLMK9G6HA42C",
        "outputId": "f7fd7834-47db-4d8d-84f8-99b67d89ad74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_preact_bn (Ba  (None, 56, 56, 64)           256       ['pool1_pool[0][0]']          \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_preact_relu (  (None, 56, 56, 64)           0         ['conv2_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4096      ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_conv[0][0]', \n",
            "                                                                     'conv2_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block2_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block3_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 28, 28, 64)           36864     ['conv2_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 28, 28, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 28, 28, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 28, 28, 256)          0         ['conv2_block2_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 28, 28, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_out (Add)      (None, 28, 28, 256)          0         ['max_pooling2d_6[0][0]',     \n",
            "                                                                     'conv2_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block1_preact_bn (Ba  (None, 28, 28, 256)          1024      ['conv2_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block1_preact_relu (  (None, 28, 28, 256)          0         ['conv3_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32768     ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_conv[0][0]', \n",
            "                                                                     'conv3_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block2_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block2_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block3_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block3_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_out (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block4_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block4_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 14, 14, 128)          147456    ['conv3_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 14, 14, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 14, 14, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 14, 14, 512)          0         ['conv3_block3_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 14, 14, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_out (Add)      (None, 14, 14, 512)          0         ['max_pooling2d_7[0][0]',     \n",
            "                                                                     'conv3_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block1_preact_bn (Ba  (None, 14, 14, 512)          2048      ['conv3_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block1_preact_relu (  (None, 14, 14, 512)          0         ['conv4_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131072    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_conv[0][0]', \n",
            "                                                                     'conv4_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block2_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block2_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block3_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block3_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block4_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block4_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block5_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block5_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block5_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block5_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block5_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block5_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block6_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block5_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block6_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block6_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block6_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block6_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 7, 7, 256)            589824    ['conv4_block6_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 7, 7, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 7, 7, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 7, 7, 1024)           0         ['conv4_block5_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 7, 7, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_out (Add)      (None, 7, 7, 1024)           0         ['max_pooling2d_8[0][0]',     \n",
            "                                                                     'conv4_block6_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block1_preact_bn (Ba  (None, 7, 7, 1024)           4096      ['conv4_block6_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block1_preact_relu (  (None, 7, 7, 1024)           0         ['conv5_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524288    ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_conv[0][0]', \n",
            "                                                                     'conv5_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block2_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block2_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block3_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block3_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " post_bn (BatchNormalizatio  (None, 7, 7, 2048)           8192      ['conv5_block3_out[0][0]']    \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " post_relu (Activation)      (None, 7, 7, 2048)           0         ['post_bn[0][0]']             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6  (None, 2048)                 0         ['post_relu[0][0]']           \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dense_72 (Dense)            (None, 1024)                 2098176   ['global_average_pooling2d_6[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 1024)                 4096      ['dense_72[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 1024)                 0         ['batch_normalization_66[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_73 (Dense)            (None, 512)                  524800    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 512)                  2048      ['dense_73[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 512)                  0         ['batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_74 (Dense)            (None, 256)                  131328    ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 256)                  1024      ['dense_74[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 256)                  0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_75 (Dense)            (None, 128)                  32896     ['activation_68[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 128)                  512       ['dense_75[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 128)                  0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_76 (Dense)            (None, 128)                  16512     ['activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 128)                  512       ['dense_76[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 128)                  0         ['batch_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_77 (Dense)            (None, 64)                   8256      ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 64)                   256       ['dense_77[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 64)                   0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_78 (Dense)            (None, 64)                   4160      ['activation_71[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 64)                   256       ['dense_78[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 64)                   0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_79 (Dense)            (None, 64)                   4160      ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 64)                   256       ['dense_79[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 64)                   0         ['batch_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_80 (Dense)            (None, 32)                   2080      ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 32)                   128       ['dense_80[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 32)                   0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_81 (Dense)            (None, 32)                   1056      ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 32)                   128       ['dense_81[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 32)                   0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_82 (Dense)            (None, 32)                   1056      ['activation_75[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 32)                   128       ['dense_82[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 32)                   0         ['batch_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_83 (Dense)            (None, 12)                   396       ['activation_76[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26399020 (100.70 MB)\n",
            "Trainable params: 2829548 (10.79 MB)\n",
            "Non-trainable params: 23569472 (89.91 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"Model(ResNet50).h5\", monitor='val_categorical_accuracy', verbose=1, mode=\"max\", save_best_only=True, save_weights_only=True)\n",
        "early = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=3)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2,\n",
        "                              mode='max', cooldown=1, patience=2, min_lr=5e-6)"
      ],
      "metadata": {
        "id": "SDKL7MrZMSJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "vDeI_GOqNr_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "UGbHlUTN4K9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpoint, early, reduce_lr],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iKwvGvxfiO3",
        "outputId": "fa871948-371a-409e-8b4d-53546bc77aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "288/288 [==============================] - ETA: 0s - loss: 39.1014 - categorical_accuracy: 0.1344\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.08333, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 3348s 12s/step - loss: 39.1014 - categorical_accuracy: 0.1344 - val_loss: 14.8908 - val_categorical_accuracy: 0.0833 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 12.3578 - categorical_accuracy: 0.1665\n",
            "Epoch 2: val_categorical_accuracy improved from 0.08333 to 0.1040, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 204s 12s/step - loss: 12.3578 - categorical_accuracy: 0.1665 - val_loss: 8.2643 - val_categorical_accuracy: 0.1040 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 8.8704 - categorical_accuracy: 0.1819\n",
            "Epoch 3: val_categorical_accuracy improved from 0.1040 to 0.1359, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 202s 12s/step - loss: 8.8704 - categorical_accuracy: 0.1819 - val_loss: 7.0168 - val_categorical_accuracy: 0.1359 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 6.9003 - categorical_accuracy: 0.2267\n",
            "Epoch 4: val_categorical_accuracy improved from 0.1359 to 0.1730, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 187s 12s/step - loss: 6.9003 - categorical_accuracy: 0.2267 - val_loss: 6.5349 - val_categorical_accuracy: 0.1730 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 5.8442 - categorical_accuracy: 0.2749\n",
            "Epoch 5: val_categorical_accuracy improved from 0.1730 to 0.2238, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 186s 12s/step - loss: 5.8442 - categorical_accuracy: 0.2749 - val_loss: 5.5711 - val_categorical_accuracy: 0.2238 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 4.7299 - categorical_accuracy: 0.3223\n",
            "Epoch 6: val_categorical_accuracy improved from 0.2238 to 0.2753, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 194s 12s/step - loss: 4.7299 - categorical_accuracy: 0.3223 - val_loss: 4.7503 - val_categorical_accuracy: 0.2753 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 3.9449 - categorical_accuracy: 0.3675\n",
            "Epoch 7: val_categorical_accuracy improved from 0.2753 to 0.3196, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 181s 11s/step - loss: 3.9449 - categorical_accuracy: 0.3675 - val_loss: 4.0908 - val_categorical_accuracy: 0.3196 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 3.7094 - categorical_accuracy: 0.4240\n",
            "Epoch 8: val_categorical_accuracy improved from 0.3196 to 0.3765, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 182s 11s/step - loss: 3.7094 - categorical_accuracy: 0.4240 - val_loss: 3.8737 - val_categorical_accuracy: 0.3765 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 3.5097 - categorical_accuracy: 0.4886\n",
            "Epoch 9: val_categorical_accuracy improved from 0.3765 to 0.4112, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 170s 12s/step - loss: 3.5097 - categorical_accuracy: 0.4886 - val_loss: 3.6883 - val_categorical_accuracy: 0.4112 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 3.2894 - categorical_accuracy: 0.5582\n",
            "Epoch 10: val_categorical_accuracy improved from 0.4112 to 0.4541, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 193s 11s/step - loss: 3.2894 - categorical_accuracy: 0.5582 - val_loss: 3.4742 - val_categorical_accuracy: 0.4541 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 3.0697 - categorical_accuracy: 0.6078\n",
            "Epoch 11: val_categorical_accuracy improved from 0.4541 to 0.4954, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 173s 12s/step - loss: 3.0697 - categorical_accuracy: 0.6078 - val_loss: 3.3387 - val_categorical_accuracy: 0.4954 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.8405 - categorical_accuracy: 0.6707\n",
            "Epoch 12: val_categorical_accuracy improved from 0.4954 to 0.5572, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 183s 11s/step - loss: 2.8405 - categorical_accuracy: 0.6707 - val_loss: 3.1585 - val_categorical_accuracy: 0.5572 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.6377 - categorical_accuracy: 0.7400\n",
            "Epoch 13: val_categorical_accuracy improved from 0.5572 to 0.6060, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 200s 11s/step - loss: 2.6377 - categorical_accuracy: 0.7400 - val_loss: 2.9842 - val_categorical_accuracy: 0.6060 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.5646 - categorical_accuracy: 0.7458\n",
            "Epoch 14: val_categorical_accuracy improved from 0.6060 to 0.6108, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 173s 12s/step - loss: 2.5646 - categorical_accuracy: 0.7458 - val_loss: 2.9293 - val_categorical_accuracy: 0.6108 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.4825 - categorical_accuracy: 0.7516\n",
            "Epoch 15: val_categorical_accuracy improved from 0.6108 to 0.6152, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 203s 12s/step - loss: 2.4825 - categorical_accuracy: 0.7516 - val_loss: 2.8775 - val_categorical_accuracy: 0.6152 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.3829 - categorical_accuracy: 0.7567\n",
            "Epoch 16: val_categorical_accuracy improved from 0.6152 to 0.6200, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 172s 12s/step - loss: 2.3829 - categorical_accuracy: 0.7567 - val_loss: 2.8199 - val_categorical_accuracy: 0.6200 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.3068 - categorical_accuracy: 0.7622\n",
            "Epoch 17: val_categorical_accuracy improved from 0.6200 to 0.6246, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 206s 11s/step - loss: 2.3068 - categorical_accuracy: 0.7622 - val_loss: 2.7528 - val_categorical_accuracy: 0.6246 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.2327 - categorical_accuracy: 0.7679\n",
            "Epoch 18: val_categorical_accuracy improved from 0.6246 to 0.6287, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 206s 12s/step - loss: 2.2327 - categorical_accuracy: 0.7679 - val_loss: 2.6784 - val_categorical_accuracy: 0.6287 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.1561 - categorical_accuracy: 0.7734\n",
            "Epoch 19: val_categorical_accuracy improved from 0.6287 to 0.6328, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 201s 11s/step - loss: 2.1561 - categorical_accuracy: 0.7734 - val_loss: 2.6236 - val_categorical_accuracy: 0.6328 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 2.0705 - categorical_accuracy: 0.7789\n",
            "Epoch 20: val_categorical_accuracy improved from 0.6328 to 0.6383, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 184s 11s/step - loss: 2.0705 - categorical_accuracy: 0.7789 - val_loss: 2.5600 - val_categorical_accuracy: 0.6383 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.9930 - categorical_accuracy: 0.7845\n",
            "Epoch 21: val_categorical_accuracy improved from 0.6383 to 0.6428, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 184s 12s/step - loss: 1.9930 - categorical_accuracy: 0.7845 - val_loss: 2.4893 - val_categorical_accuracy: 0.6428 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.9000 - categorical_accuracy: 0.7902\n",
            "Epoch 22: val_categorical_accuracy improved from 0.6428 to 0.6484, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 180s 11s/step - loss: 1.9000 - categorical_accuracy: 0.7902 - val_loss: 2.4411 - val_categorical_accuracy: 0.6484 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.8213 - categorical_accuracy: 0.7955\n",
            "Epoch 23: val_categorical_accuracy improved from 0.6484 to 0.6528, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 183s 12s/step - loss: 1.8213 - categorical_accuracy: 0.7955 - val_loss: 2.3946 - val_categorical_accuracy: 0.6528 - lr: 0.001\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.7289 - categorical_accuracy: 0.8012\n",
            "Epoch 24: val_categorical_accuracy improved from 0.6528 to 0.6575, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 183s 12s/step - loss: 1.7289 - categorical_accuracy: 0.8012 - val_loss: 2.3298 - val_categorical_accuracy: 0.6575 - lr: 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dxDipUZxRj4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "52c79fb7-5f22-47cf-f267-b08fcb6e931d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-KAgUAGmRrVd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "7d62ab0c-4c38-4c0d-ac8b-e44ebd02efe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE TUNING"
      ],
      "metadata": {
        "id": "-JMHntDfroUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "regularizetion_rate = 5e-5\n",
        "dropout_rate = 0.2\n",
        "learning_rate = 5e-5"
      ],
      "metadata": {
        "id": "lhFwhui9r0Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_trainable = False\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = is_trainable\n",
        "\n",
        "    if layer.name == \"conv4_block6_out\":\n",
        "        is_trainable = True"
      ],
      "metadata": {
        "id": "95sj2lvfrnBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0iUGqsGr-Ip",
        "outputId": "3608de5a-0488-4520-f5b4-5473011e9a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)   (None, 230, 230, 3)          0         ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)         (None, 112, 112, 64)         9472      ['conv1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)   (None, 114, 114, 64)         0         ['conv1_conv[0][0]']          \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)   (None, 56, 56, 64)           0         ['pool1_pad[0][0]']           \n",
            "                                                                                                  \n",
            " conv2_block1_preact_bn (Ba  (None, 56, 56, 64)           256       ['pool1_pool[0][0]']          \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block1_preact_relu (  (None, 56, 56, 64)           0         ['conv2_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2  (None, 56, 56, 64)           4096      ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_0_conv[0][0]', \n",
            "                                                                     'conv2_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block2_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block2_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2  (None, 56, 56, 64)           36864     ['conv2_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2  (None, 56, 56, 256)          16640     ['conv2_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_out (Add)      (None, 56, 56, 256)          0         ['conv2_block1_out[0][0]',    \n",
            "                                                                     'conv2_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv2_block3_preact_bn (Ba  (None, 56, 56, 256)          1024      ['conv2_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv2_block3_preact_relu (  (None, 56, 56, 256)          0         ['conv2_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2  (None, 56, 56, 64)           16384     ['conv2_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNo  (None, 56, 56, 64)           256       ['conv2_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activ  (None, 56, 56, 64)           0         ['conv2_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv2_block3_2_pad (ZeroPa  (None, 58, 58, 64)           0         ['conv2_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2  (None, 28, 28, 64)           36864     ['conv2_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNo  (None, 28, 28, 64)           256       ['conv2_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activ  (None, 28, 28, 64)           0         ['conv2_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_6 (MaxPoolin  (None, 28, 28, 256)          0         ['conv2_block2_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2  (None, 28, 28, 256)          16640     ['conv2_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_out (Add)      (None, 28, 28, 256)          0         ['max_pooling2d_6[0][0]',     \n",
            "                                                                     'conv2_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block1_preact_bn (Ba  (None, 28, 28, 256)          1024      ['conv2_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block1_preact_relu (  (None, 28, 28, 256)          0         ['conv3_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2  (None, 28, 28, 128)          32768     ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2  (None, 28, 28, 512)          131584    ['conv3_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_0_conv[0][0]', \n",
            "                                                                     'conv3_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block2_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block2_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_out (Add)      (None, 28, 28, 512)          0         ['conv3_block1_out[0][0]',    \n",
            "                                                                     'conv3_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block3_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block3_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2  (None, 28, 28, 128)          147456    ['conv3_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2  (None, 28, 28, 512)          66048     ['conv3_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_out (Add)      (None, 28, 28, 512)          0         ['conv3_block2_out[0][0]',    \n",
            "                                                                     'conv3_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv3_block4_preact_bn (Ba  (None, 28, 28, 512)          2048      ['conv3_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv3_block4_preact_relu (  (None, 28, 28, 512)          0         ['conv3_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2  (None, 28, 28, 128)          65536     ['conv3_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNo  (None, 28, 28, 128)          512       ['conv3_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activ  (None, 28, 28, 128)          0         ['conv3_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv3_block4_2_pad (ZeroPa  (None, 30, 30, 128)          0         ['conv3_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2  (None, 14, 14, 128)          147456    ['conv3_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNo  (None, 14, 14, 128)          512       ['conv3_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activ  (None, 14, 14, 128)          0         ['conv3_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_7 (MaxPoolin  (None, 14, 14, 512)          0         ['conv3_block3_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2  (None, 14, 14, 512)          66048     ['conv3_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_out (Add)      (None, 14, 14, 512)          0         ['max_pooling2d_7[0][0]',     \n",
            "                                                                     'conv3_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block1_preact_bn (Ba  (None, 14, 14, 512)          2048      ['conv3_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block1_preact_relu (  (None, 14, 14, 512)          0         ['conv4_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2  (None, 14, 14, 256)          131072    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2  (None, 14, 14, 1024)         525312    ['conv4_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_0_conv[0][0]', \n",
            "                                                                     'conv4_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block2_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block2_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block1_out[0][0]',    \n",
            "                                                                     'conv4_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block3_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block3_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block2_out[0][0]',    \n",
            "                                                                     'conv4_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block4_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block3_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block4_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block4_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block4_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block4_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block4_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block4_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block4_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block4_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block3_out[0][0]',    \n",
            "                                                                     'conv4_block4_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block5_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block4_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block5_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block5_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block5_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block5_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2  (None, 14, 14, 256)          589824    ['conv4_block5_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block5_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block5_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2  (None, 14, 14, 1024)         263168    ['conv4_block5_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_out (Add)      (None, 14, 14, 1024)         0         ['conv4_block4_out[0][0]',    \n",
            "                                                                     'conv4_block5_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv4_block6_preact_bn (Ba  (None, 14, 14, 1024)         4096      ['conv4_block5_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv4_block6_preact_relu (  (None, 14, 14, 1024)         0         ['conv4_block6_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2  (None, 14, 14, 256)          262144    ['conv4_block6_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNo  (None, 14, 14, 256)          1024      ['conv4_block6_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activ  (None, 14, 14, 256)          0         ['conv4_block6_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv4_block6_2_pad (ZeroPa  (None, 16, 16, 256)          0         ['conv4_block6_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2  (None, 7, 7, 256)            589824    ['conv4_block6_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNo  (None, 7, 7, 256)            1024      ['conv4_block6_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activ  (None, 7, 7, 256)            0         ['conv4_block6_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPoolin  (None, 7, 7, 1024)           0         ['conv4_block5_out[0][0]']    \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2  (None, 7, 7, 1024)           263168    ['conv4_block6_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_out (Add)      (None, 7, 7, 1024)           0         ['max_pooling2d_8[0][0]',     \n",
            "                                                                     'conv4_block6_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block1_preact_bn (Ba  (None, 7, 7, 1024)           4096      ['conv4_block6_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block1_preact_relu (  (None, 7, 7, 1024)           0         ['conv5_block1_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2  (None, 7, 7, 512)            524288    ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block1_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block1_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block1_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block1_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2  (None, 7, 7, 2048)           2099200   ['conv5_block1_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block1_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_0_conv[0][0]', \n",
            "                                                                     'conv5_block1_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block2_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block1_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block2_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block2_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block2_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block2_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block2_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block2_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block2_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block2_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block1_out[0][0]',    \n",
            "                                                                     'conv5_block2_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " conv5_block3_preact_bn (Ba  (None, 7, 7, 2048)           8192      ['conv5_block2_out[0][0]']    \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " conv5_block3_preact_relu (  (None, 7, 7, 2048)           0         ['conv5_block3_preact_bn[0][0]\n",
            " Activation)                                                        ']                            \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2  (None, 7, 7, 512)            1048576   ['conv5_block3_preact_relu[0][\n",
            " D)                                                                 0]']                          \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_1_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_1_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_2_pad (ZeroPa  (None, 9, 9, 512)            0         ['conv5_block3_1_relu[0][0]'] \n",
            " dding2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2  (None, 7, 7, 512)            2359296   ['conv5_block3_2_pad[0][0]']  \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNo  (None, 7, 7, 512)            2048      ['conv5_block3_2_conv[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activ  (None, 7, 7, 512)            0         ['conv5_block3_2_bn[0][0]']   \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2  (None, 7, 7, 2048)           1050624   ['conv5_block3_2_relu[0][0]'] \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_out (Add)      (None, 7, 7, 2048)           0         ['conv5_block2_out[0][0]',    \n",
            "                                                                     'conv5_block3_3_conv[0][0]'] \n",
            "                                                                                                  \n",
            " post_bn (BatchNormalizatio  (None, 7, 7, 2048)           8192      ['conv5_block3_out[0][0]']    \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " post_relu (Activation)      (None, 7, 7, 2048)           0         ['post_bn[0][0]']             \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6  (None, 2048)                 0         ['post_relu[0][0]']           \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            " dense_72 (Dense)            (None, 1024)                 2098176   ['global_average_pooling2d_6[0\n",
            "                                                                    ][0]']                        \n",
            "                                                                                                  \n",
            " batch_normalization_66 (Ba  (None, 1024)                 4096      ['dense_72[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_66 (Activation)  (None, 1024)                 0         ['batch_normalization_66[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_73 (Dense)            (None, 512)                  524800    ['activation_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_67 (Ba  (None, 512)                  2048      ['dense_73[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_67 (Activation)  (None, 512)                  0         ['batch_normalization_67[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_74 (Dense)            (None, 256)                  131328    ['activation_67[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_68 (Ba  (None, 256)                  1024      ['dense_74[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_68 (Activation)  (None, 256)                  0         ['batch_normalization_68[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_75 (Dense)            (None, 128)                  32896     ['activation_68[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_69 (Ba  (None, 128)                  512       ['dense_75[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_69 (Activation)  (None, 128)                  0         ['batch_normalization_69[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_76 (Dense)            (None, 128)                  16512     ['activation_69[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_70 (Ba  (None, 128)                  512       ['dense_76[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_70 (Activation)  (None, 128)                  0         ['batch_normalization_70[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_77 (Dense)            (None, 64)                   8256      ['activation_70[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_71 (Ba  (None, 64)                   256       ['dense_77[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_71 (Activation)  (None, 64)                   0         ['batch_normalization_71[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_78 (Dense)            (None, 64)                   4160      ['activation_71[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_72 (Ba  (None, 64)                   256       ['dense_78[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_72 (Activation)  (None, 64)                   0         ['batch_normalization_72[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_79 (Dense)            (None, 64)                   4160      ['activation_72[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_73 (Ba  (None, 64)                   256       ['dense_79[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_73 (Activation)  (None, 64)                   0         ['batch_normalization_73[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_80 (Dense)            (None, 32)                   2080      ['activation_73[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_74 (Ba  (None, 32)                   128       ['dense_80[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_74 (Activation)  (None, 32)                   0         ['batch_normalization_74[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_81 (Dense)            (None, 32)                   1056      ['activation_74[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_75 (Ba  (None, 32)                   128       ['dense_81[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_75 (Activation)  (None, 32)                   0         ['batch_normalization_75[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_82 (Dense)            (None, 32)                   1056      ['activation_75[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_76 (Ba  (None, 32)                   128       ['dense_82[0][0]']            \n",
            " tchNormalization)                                                                                \n",
            "                                                                                                  \n",
            " activation_76 (Activation)  (None, 32)                   0         ['batch_normalization_76[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense_83 (Dense)            (None, 12)                   396       ['activation_76[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 26399020 (100.70 MB)\n",
            "Trainable params: 17800428 (67.90 MB)\n",
            "Non-trainable params: 8598592 (32.80 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %32 sini dondurdum sadece son conv blockları eğitiyorum"
      ],
      "metadata": {
        "id": "_u0bo-Uws5zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(\"Model(ResNet50).h5\", monitor='val_categorical_accuracy', verbose=1, mode=\"max\", save_best_only=True, save_weights_only=True)\n",
        "early = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=3)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.2,\n",
        "                              mode='max', cooldown=1, patience=2, min_lr=5e-6)"
      ],
      "metadata": {
        "id": "oLCMBBTGsJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "WDxrDSl7sN5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "m0CbAGmksQzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpoint, early, reduce_lr],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaSY7sh3taFP",
        "outputId": "33c2d14d-c67d-46ec-dfc5-51a8d2748e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.7623 - categorical_accuracy: 0.8164\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.7432, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 501s 12s/step - loss: 1.7623 - categorical_accuracy: 0.8164 - val_loss: 1.9369 - val_categorical_accuracy: 0.7432 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.5963 - categorical_accuracy: 0.8296\n",
            "Epoch 2: val_categorical_accuracy improved from 0.7432 to 0.7601, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 387s 22s/step - loss: 1.5963 - categorical_accuracy: 0.8296 - val_loss: 1.7908 - val_categorical_accuracy: 0.7601 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.4849 - categorical_accuracy: 0.8428\n",
            "Epoch 3: val_categorical_accuracy improved from 0.7601 to 0.7756, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 369s 23s/step - loss: 1.4849 - categorical_accuracy: 0.8428 - val_loss: 1.6740 - val_categorical_accuracy: 0.7756 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.3218 - categorical_accuracy: 0.8606\n",
            "Epoch 4: val_categorical_accuracy improved from 0.7756 to 0.7925, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 388s 23s/step - loss: 1.3218 - categorical_accuracy: 0.8606 - val_loss: 1.5439 - val_categorical_accuracy: 0.7925 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.2062 - categorical_accuracy: 0.8758\n",
            "Epoch 5: val_categorical_accuracy improved from 0.7925 to 0.8081, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 364s 23s/step - loss: 1.2062 - categorical_accuracy: 0.8758 - val_loss: 1.3949 - val_categorical_accuracy: 0.8081 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0515 - categorical_accuracy: 0.8935\n",
            "Epoch 6: val_categorical_accuracy improved from 0.8081 to 0.8229, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 382s 23s/step - loss: 1.0515 - categorical_accuracy: 0.8935 - val_loss: 1.2854 - val_categorical_accuracy: 0.8229 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0451 - categorical_accuracy: 0.8941\n",
            "Epoch 7: val_categorical_accuracy improved from 0.8229 to 0.8236, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 373s 23s/step - loss: 1.0451 - categorical_accuracy: 0.8941 - val_loss: 1.2793 - val_categorical_accuracy: 0.8236 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0395 - categorical_accuracy: 0.8945\n",
            "Epoch 8: val_categorical_accuracy improved from 0.8236 to 0.8240, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 362s 23s/step - loss: 1.0395 - categorical_accuracy: 0.8945 - val_loss: 1.2772 - val_categorical_accuracy: 0.8240 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0337 - categorical_accuracy: 0.8948\n",
            "Epoch 9: val_categorical_accuracy improved from 0.8240 to 0.8247, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 370s 23s/step - loss: 1.0337 - categorical_accuracy: 0.8948 - val_loss: 1.2728 - val_categorical_accuracy: 0.8247 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0278 - categorical_accuracy: 0.8955\n",
            "Epoch 10: val_categorical_accuracy improved from 0.8247 to 0.8251, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 397s 22s/step - loss: 1.0278 - categorical_accuracy: 0.8955 - val_loss: 1.2691 - val_categorical_accuracy: 0.8251 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0222 - categorical_accuracy: 0.8959\n",
            "Epoch 11: val_categorical_accuracy improved from 0.8251 to 0.8253, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 360s 22s/step - loss: 1.0222 - categorical_accuracy: 0.8959 - val_loss: 1.2631 - val_categorical_accuracy: 0.8253 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0180 - categorical_accuracy: 0.8967\n",
            "Epoch 12: val_categorical_accuracy improved from 0.8253 to 0.8261, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 367s 23s/step - loss: 1.0180 - categorical_accuracy: 0.8967 - val_loss: 1.2582 - val_categorical_accuracy: 0.8261 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0110 - categorical_accuracy: 0.8973\n",
            "Epoch 13: val_categorical_accuracy improved from 0.8261 to 0.8267, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 396s 23s/step - loss: 1.0110 - categorical_accuracy: 0.8973 - val_loss: 1.2548 - val_categorical_accuracy: 0.8267 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 1.0063 - categorical_accuracy: 0.8976\n",
            "Epoch 14: val_categorical_accuracy improved from 0.8267 to 0.8270, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 370s 23s/step - loss: 1.0063 - categorical_accuracy: 0.8976 - val_loss: 1.2505 - val_categorical_accuracy: 0.8270 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9999 - categorical_accuracy: 0.8980\n",
            "Epoch 15: val_categorical_accuracy improved from 0.8270 to 0.8277, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 380s 22s/step - loss: 0.9999 - categorical_accuracy: 0.8980 - val_loss: 1.2445 - val_categorical_accuracy: 0.8277 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9930 - categorical_accuracy: 0.8987\n",
            "Epoch 16: val_categorical_accuracy improved from 0.8277 to 0.8283, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 397s 22s/step - loss: 0.9930 - categorical_accuracy: 0.8987 - val_loss: 1.2415 - val_categorical_accuracy: 0.8283 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9874 - categorical_accuracy: 0.8994\n",
            "Epoch 17: val_categorical_accuracy improved from 0.8283 to 0.8288, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 392s 23s/step - loss: 0.9874 - categorical_accuracy: 0.8994 - val_loss: 1.2364 - val_categorical_accuracy: 0.8288 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9829 - categorical_accuracy: 0.9000\n",
            "Epoch 18: val_categorical_accuracy improved from 0.8288 to 0.8296, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 365s 23s/step - loss: 0.9829 - categorical_accuracy: 0.9000 - val_loss: 1.2313 - val_categorical_accuracy: 0.8296 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9778 - categorical_accuracy: 0.9007\n",
            "Epoch 19: val_categorical_accuracy improved from 0.8296 to 0.8298, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 379s 23s/step - loss: 0.9778 - categorical_accuracy: 0.9007 - val_loss: 1.2254 - val_categorical_accuracy: 0.8298 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9717 - categorical_accuracy: 0.9012\n",
            "Epoch 20: val_categorical_accuracy improved from 0.8298 to 0.8301, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 373s 22s/step - loss: 0.9717 - categorical_accuracy: 0.9012 - val_loss: 1.2212 - val_categorical_accuracy: 0.8301 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9666 - categorical_accuracy: 0.9019\n",
            "Epoch 21: val_categorical_accuracy improved from 0.8301 to 0.8304, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 368s 23s/step - loss: 0.9666 - categorical_accuracy: 0.9019 - val_loss: 1.2180 - val_categorical_accuracy: 0.8304 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9615 - categorical_accuracy: 0.9024\n",
            "Epoch 22: val_categorical_accuracy improved from 0.8304 to 0.8308, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 384s 23s/step - loss: 0.9615 - categorical_accuracy: 0.9024 - val_loss: 1.2118 - val_categorical_accuracy: 0.8308 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9563 - categorical_accuracy: 0.9030\n",
            "Epoch 23: val_categorical_accuracy improved from 0.8308 to 0.8317, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 392s 22s/step - loss: 0.9563 - categorical_accuracy: 0.9030 - val_loss: 1.2077 - val_categorical_accuracy: 0.8317 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9508 - categorical_accuracy: 0.9036\n",
            "Epoch 24: val_categorical_accuracy improved from 0.8317 to 0.8321, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 396s 23s/step - loss: 0.9508 - categorical_accuracy: 0.9036 - val_loss: 1.2023 - val_categorical_accuracy: 0.8321 - lr: 0.00005\n",
            "288/288 [==============================] - ETA: 0s - loss: 0.9447 - categorical_accuracy: 0.9043\n",
            "Epoch 25: val_categorical_accuracy improved from 0.8321 to 0.8328, saving model to Model(ResNet50).h5\n",
            "288/288 [==============================] - 378s 23s/step - loss: 0.9447 - categorical_accuracy: 0.9043 - val_loss: 1.1982 - val_categorical_accuracy: 0.8328 - lr: 0.00005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bfOR0CjU6aKh",
        "outputId": "30092092-bb3b-4183-a08e-976c46b43d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bnkxYCOs6fHR",
        "outputId": "1f12a190-82a1-43c8-ca1d-d0f78a4167d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VjpwV3UDZ7Z",
        "outputId": "32245ce9-5ae4-4aee-9759-9fb6649b810c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 154s 12s/step - loss: 1.1312 - categorical_accuracy: 0.8417\n",
            "[1.131276495167320, 0.841706732946855]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FC katmanları eklendikten sonra eğitimden gelen model overfit'li bir halde geldiği için fine tune yaparken de overfit hali devam ediyor. Bunu train loss grafiğinin validation'dan çok aşağıda, accuracy grafiğinde de çok yukarıda oluşundan anlıyoruz. Bunu ekstradan dropout katmanı kullanarak çözebiliriz(aslında dropout ile de denemiştim hocam ama gidişatı daha kötü hissettirdi bana o yüzden en son model dropoutsuz oldu.). Belki tüm model unfreeze edilip tekrardan o en baştaki conv blocklarıyla beraber fine tune edilebilir ama ben çok bir etkisi olacağını düşünmüyorum."
      ],
      "metadata": {
        "id": "2-KuXvaV_63U"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}